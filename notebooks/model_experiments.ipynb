{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments\n",
    "\n",
    "This notebook is for experimenting with the choice of model and performance variations between different time periods and clinics.\n",
    "\n",
    "1. Train and evaluate current best model on total dataset (without differentiating between clinics)\n",
    "2. Apply IECV to different models and check heteregeneity with respect to clinics\n",
    "3. Train models per clinic and check performance\n",
    "4. Use clinic as categorical feature in tree based models and check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedGroupKFold,\n",
    "    TimeSeriesSplit,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, SplineTransformer\n",
    "\n",
    "from noshow.features.feature_pipeline import create_features\n",
    "from noshow.model.predict import create_prediction\n",
    "from noshow.preprocessing.load_data import (\n",
    "    load_appointment_csv,\n",
    "    process_appointments,\n",
    "    process_postal_codes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split in to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuretable = pd.read_parquet(\"../data/processed/featuretable.parquet\")\n",
    "\n",
    "featuretable[\"no_show\"] = (\n",
    "    featuretable[\"no_show\"].replace({\"no_show\": \"1\", \"show\": \"0\"}).astype(int)\n",
    ")\n",
    "featuretable[\"hour\"] = featuretable[\"hour\"].astype(\"category\")\n",
    "featuretable[\"weekday\"] = featuretable[\"weekday\"].astype(\"category\")\n",
    "\n",
    "print(featuretable.dtypes)\n",
    "\n",
    "X, y = featuretable.drop(columns=\"no_show\"), featuretable[\"no_show\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgboost_model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    categorical_features=[\"hour\", \"weekday\"],\n",
    ")\n",
    "\n",
    "categorical_features = [\"hour\", \"weekday\"]\n",
    "continuous_features = X.columns.difference(categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"continuous\",\n",
    "            Pipeline([(\"scaler\", RobustScaler()), (\"spline\", SplineTransformer())]),\n",
    "            continuous_features,\n",
    "        ),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate current best model on total dataset (without differentiating between clinics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_auc_curve(X_train, y_train, model, cv, train_groups=None, title=None):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    test_indices = {}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train, train_groups)):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_score = model.predict_proba(X_test_cv)[:, 1]\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_cv, y_score)\n",
    "        roc_auc[i] = roc_auc_score(y_test_cv, y_score)\n",
    "        test_indices[i] = test_idx\n",
    "\n",
    "        ax.plot(fpr[i], tpr[i], c=\"b\", alpha=0.15)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    # Add mean AUC and standard deviation to the legend\n",
    "    mean_auc = np.mean(list(roc_auc.values()))\n",
    "    std_auc = np.std(list(roc_auc.values()))\n",
    "    ax.legend([f\"ROC curve (AUC = {mean_auc:.3f} +/- {std_auc:.3f})\"])\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    fig.show()\n",
    "\n",
    "    return roc_auc, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    lgboost_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    log_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "# load the appointments data\n",
    "appointments_df = load_appointment_csv(\"../data/raw/poliafspraken_pilot.csv\")\n",
    "appointments_df = process_appointments(appointments_df).sort_index()\n",
    "with open(\"../output/models/no_show_model_cv.pickle\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# create prediction scores and only select year 2022\n",
    "all_postalcodes = process_postal_codes(\"../data/raw/NL.txt\")\n",
    "predictions_df = create_prediction(model, appointments_df, all_postalcodes)\n",
    "predictions_df = predictions_df.loc[\n",
    "    predictions_df.index.get_level_values(\"start\").year == 2022\n",
    "]\n",
    "predictions_df = predictions_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy hoofdagendas with the requirement (97% of same hoofdagenda for each pseudo_id)\n",
    "groups = [\"A\", \"B\", \"C\"]\n",
    "required_percentage = 0.97\n",
    "\n",
    "def assign_group(df, groups, required_percentage):\n",
    "    # Calculate the number of rows needed to satisfy the required percentage\n",
    "    count = int(np.ceil(required_percentage * len(df)))\n",
    "    # Choose a main group for the majority of entries\n",
    "    main_group = np.random.choice(groups)\n",
    "    # Assign the main group to the required percentage of rows\n",
    "    df['hoofdagenda'] = main_group\n",
    "\n",
    "    # Optionally, assign other groups to remaining rows\n",
    "    remaining_indices = df.index[count:]  # indices for remaining rows\n",
    "    if len(remaining_indices) > 0:\n",
    "        df.loc[remaining_indices, 'hoofdagenda'] = np.random.choice(\n",
    "            [g for g in groups if g != main_group], len(remaining_indices))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to each group of pseudo_id\n",
    "predictions_df = predictions_df.groupby('pseudo_id').apply(assign_group, groups=groups,\n",
    "                                                            required_percentage=required_percentage).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_df.shape)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create modified version of create_treatment group to test stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Function to apply the appropriate bin edges to each group\n",
    "def apply_bins(group, bin_dict):\n",
    "    edges = bin_dict[group.name]\n",
    "    # Use pd.cut to segment the prediction values into bins based on the edges\n",
    "    # 'labels=False' will return the indices of the bins from 0 to n_bins-1\n",
    "    group['score_bin'] = pd.cut(group['prediction'],\n",
    "                                 bins=[0] + list(edges.values())[1:] + [1],\n",
    "                                   labels=False)\n",
    "    return group\n",
    "\n",
    "# create modified version of create_treatment_groups\n",
    "def create_treatment_groups(\n",
    "    predictions: pd.DataFrame, patients: pd.DataFrame, bin_edges\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create treatment groups based on predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : pd.DataFrame\n",
    "        DataFrame containing predictions.\n",
    "    patients : pd.DataFrame\n",
    "        A dataframe containing the treatment groups of patients\n",
    "    bin_edges : list\n",
    "        List of edges defining the bins for prediction scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with treatment group assignments.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the predictions DataFrame is empty.\n",
    "    \"\"\"\n",
    "    if predictions.empty:\n",
    "        raise ValueError(\"The predictions DataFrame is empty.\")\n",
    "\n",
    "    # get unique patient ids\n",
    "    unique_patient_ids = predictions[\"pseudo_id\"].unique().tolist()\n",
    "    relevant_patients = patients[patients[\"id\"].isin(unique_patient_ids)]\n",
    "\n",
    "    if not relevant_patients.empty:\n",
    "        # Merge predictions with patients to get treatment group\n",
    "        predictions = pd.merge(\n",
    "            predictions,\n",
    "            relevant_patients[[\"id\", \"treatment_group\"]],\n",
    "            right_on=\"id\",\n",
    "            left_on=\"pseudo_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        predictions.drop(columns=[\"id\"], inplace=True)\n",
    "    else:\n",
    "        predictions.loc[:, \"treatment_group\"] = None\n",
    "\n",
    "    predictions = predictions.groupby('hoofdagenda').apply(apply_bins,\n",
    "                                                            bin_dict=bin_edges,\n",
    "                                                            include_groups=False).reset_index()\n",
    "    predictions = predictions.drop(columns=\"level_1\")\n",
    "    predictions = predictions.sort_values([\"prediction\"], ascending=False)\n",
    "\n",
    "    # Fill NaN values in 'treatment_group' with calculated values\n",
    "    mask = predictions[\"treatment_group\"].isnull()\n",
    "    predictions.loc[mask, 'treatment_group'] = (\n",
    "        predictions[mask]\n",
    "        .groupby(['hoofdagenda', 'score_bin'])['prediction']\n",
    "        .transform(lambda x: (np.arange(len(x)) + random.randint(0, 1)) % 2)\n",
    "    )\n",
    "\n",
    "    # Apply mode calculation\n",
    "    predictions.loc[mask, 'treatment_group'] = (\n",
    "        predictions[mask]\n",
    "        .groupby('pseudo_id')['treatment_group']\n",
    "        .transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
    "    )\n",
    "\n",
    "    # if new patients exist add them to the already existing patient table\n",
    "    if len(predictions[mask]) > 0:\n",
    "        new_patients = predictions[mask].drop_duplicates(subset='pseudo_id').copy()\n",
    "        new_patients = new_patients.rename(columns={'pseudo_id': 'id'}).reset_index(drop=True)\n",
    "        patients = pd.merge(patients, new_patients[['id', 'treatment_group']], on='id', how='outer', suffixes=('_df1', '_df2'))\n",
    "        patients['treatment_group'] = patients['treatment_group_df1'].fillna(patients['treatment_group_df2'])\n",
    "        patients.drop(columns=['treatment_group_df1', 'treatment_group_df2'], inplace=True)\n",
    "    return predictions, patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prediction score bins for every first appointment of every patient and plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantiles\n",
    "n_bins = 4\n",
    "quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "# determine quantiles for every hoofdagenda group in preditcions_df\n",
    "bin_edges = (predictions_df.sort_values(\"prediction\", ascending=False)\n",
    "             .drop_duplicates(subset=\"pseudo_id\", keep=\"first\")\n",
    "             .groupby('hoofdagenda')['prediction'].quantile(quantiles).reset_index())\n",
    "\n",
    "bin_edges = pd.pivot_table(bin_edges, values='prediction', index='hoofdagenda', columns='level_1')\n",
    "# create a dict where hoodagendas are keys and bin_edges for the quantiles are values\n",
    "bin_edges = bin_edges.to_dict(orient='index')\n",
    "print(bin_edges)\n",
    "\n",
    "# Apply the function across the dataframe, grouped by 'hoofdagenda'\n",
    "check = predictions_df.groupby('hoofdagenda').apply(apply_bins, bin_dict=bin_edges)\n",
    "check = check.reset_index(drop=True)\n",
    "# # plot distribution of each bin\n",
    "fig, ax = plt.subplots()\n",
    "check.groupby([\"hoofdagenda\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Overal distribution of Treatment Groups per hoofdagenda and score_bin\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "check.sort_values(\"prediction\", ascending=False).drop_duplicates(subset=\"pseudo_id\", keep=\"first\").groupby([\"hoofdagenda\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Treatment Groups per hoofdagenda and score_bin for first appointment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the stratification and visualize new distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every data\n",
    "days = predictions_df[\"start\"].dt.date.unique()\n",
    "patients = pd.DataFrame(columns=[\"id\", \"treatment_group\"])\n",
    "final_predictions = []\n",
    "for day in days:\n",
    "    day_df = predictions_df[predictions_df[\"start\"].dt.date == day]\n",
    "    predict, patients = create_treatment_groups(day_df, patients, bin_edges)\n",
    "    final_predictions.append(predict)\n",
    "\n",
    "final_predictions= pd.concat(final_predictions)\n",
    "final_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that every pseudo_id have a single unique treatment group\n",
    "assert final_predictions.groupby('pseudo_id')['treatment_group'].nunique().eq(1).all()\n",
    "\n",
    "# assert every pseudo_id has a treatment group\n",
    "assert final_predictions['treatment_group'].notnull().all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first entry for every pseuoo_id\n",
    "first_app = final_predictions.sort_values(\"prediction\", ascending=False).drop_duplicates(subset=\"pseudo_id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "first_app.groupby([\"hoofdagenda\", \"treatment_group\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Treatment Groups per hoofdagenda and score_bin for first appointment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "final_predictions.groupby([\"hoofdagenda\", \"treatment_group\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Overall distribution of Treatment Groups per hoofdagenda and score_bin\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check temporal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_timesorted = X.sort_index(level=\"start\")\n",
    "y_timesorted = y.sort_index(level=\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, test_indices = cv_auc_curve(\n",
    "    X_timesorted,\n",
    "    y_timesorted,\n",
    "    lgboost_model,\n",
    "    TimeSeriesSplit(n_splits=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_times = [\n",
    "    str(\n",
    "        (\n",
    "            X.iloc[idx].index.get_level_values(\"start\").min().strftime(\"%Y-%m-%d\"),\n",
    "            X.iloc[idx].index.get_level_values(\"start\").max().strftime(\"%Y-%m-%d\"),\n",
    "        )\n",
    "    )\n",
    "    for idx in test_indices.values()\n",
    "]\n",
    "fold_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_scores = pd.Series(roc_auc)\n",
    "roc_scores.index = fold_times\n",
    "roc_scores.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply IECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_df = load_appointment_csv(\"../data/raw/poliafspraken_no_show.csv\")\n",
    "appointments_df = process_appointments(appointments_df)\n",
    "all_postalcodes = process_postal_codes(\"../data/raw/NL.txt\")\n",
    "appointments_features = create_features(\n",
    "    appointments_df, all_postalcodes, minutes_early_cutoff=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features = (\n",
    "    appointments_features[\n",
    "        [\n",
    "            \"hoofdagenda\",\n",
    "            \"hour\",\n",
    "            \"weekday\",\n",
    "            \"minutesDuration\",\n",
    "            \"no_show\",\n",
    "            \"prev_no_show\",\n",
    "            \"prev_no_show_perc\",\n",
    "            \"age\",\n",
    "            \"dist_umcu\",\n",
    "            \"prev_minutes_early\",\n",
    "            \"earlier_appointments\",\n",
    "            \"appointments_same_day\",\n",
    "            \"appointments_last_days\",\n",
    "            \"days_since_created\",\n",
    "            \"days_since_last_appointment\",\n",
    "        ]\n",
    "    ]\n",
    "    .reset_index()\n",
    "    .set_index([\"pseudo_id\", \"start\", \"hoofdagenda\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features[\"no_show\"] = (\n",
    "    appointments_features[\"no_show\"].replace({\"no_show\": 1, \"show\": 0}).astype(int)\n",
    ")\n",
    "\n",
    "appointments_features[\"hour\"] = appointments_features[\"hour\"].astype(\"category\")\n",
    "appointments_features[\"weekday\"] = appointments_features[\"weekday\"].astype(\"category\")\n",
    "\n",
    "X, y = appointments_features.drop(columns=\"no_show\"), appointments_features[\"no_show\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = X.index.get_level_values(\"pseudo_id\")\n",
    "\n",
    "cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    lgboost_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_leave_one_out(df):\n",
    "    groups = df.index.get_level_values(\"hoofdagenda\").unique()\n",
    "\n",
    "    for test_group in groups:\n",
    "        train_index = df.index.get_level_values(\"hoofdagenda\") != test_group\n",
    "        test_index = df.index.get_level_values(\"hoofdagenda\") == test_group\n",
    "\n",
    "        yield test_group, (np.where(train_index)[0], np.where(test_index)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iecv_auc_curve(X_train, y_train, model):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    test_indices = {}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for group, (train_idx, test_idx) in group_leave_one_out(X_train):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_score = model.predict_proba(X_test_cv)[:, 1]\n",
    "        fpr[group], tpr[group], _ = roc_curve(y_test_cv, y_score)\n",
    "        roc_auc[group] = roc_auc_score(y_test_cv, y_score)\n",
    "        test_indices[group] = test_idx\n",
    "\n",
    "        ax.plot(fpr[group], tpr[group], label=group)\n",
    "\n",
    "    print(\n",
    "        f\"Mean AUC: {np.mean(list(roc_auc.values()))}\"\n",
    "        f\"(+/- {np.std(list(roc_auc.values()))})\"\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "    return roc_auc, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iecv_auc_curve(X, y, lgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iecv_auc_curve(X, y, log_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV per poli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = appointments_features.drop(columns=\"no_show\"), appointments_features[\"no_show\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poli in X.index.get_level_values(\"hoofdagenda\").unique():\n",
    "    X_tmp = X.loc[(slice(None), slice(None), poli), :]\n",
    "    y_tmp = y.loc[(slice(None), slice(None), poli)]\n",
    "    train_groups_tmp = X_tmp.index.get_level_values(\"pseudo_id\")\n",
    "\n",
    "    _, _ = cv_auc_curve(\n",
    "        X_tmp,\n",
    "        y_tmp,\n",
    "        lgboost_model,\n",
    "        # HistGradientBoostingClassifier(learning_rate=0.05, max_iter=300),\n",
    "        StratifiedGroupKFold(n_splits=5),\n",
    "        train_groups_tmp,\n",
    "        title=poli,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding poli as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features_agenda = appointments_features.reset_index()\n",
    "appointments_features_agenda[\"hoofdagenda_cat\"] = appointments_features_agenda[\n",
    "    \"hoofdagenda\"\n",
    "].astype(\"category\")\n",
    "appointments_features_agenda = appointments_features_agenda.set_index(\n",
    "    [\"pseudo_id\", \"start\", \"hoofdagenda\"]\n",
    ")\n",
    "X, y = (\n",
    "    appointments_features_agenda.drop(columns=\"no_show\"),\n",
    "    appointments_features_agenda[\"no_show\"],\n",
    ")\n",
    "\n",
    "model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    categorical_features=[\"hour\", \"weekday\", \"hoofdagenda_cat\"],\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = featuretable.drop(columns=\"no_show\"), featuretable[\"no_show\"]\n",
    "\n",
    "model = log_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(log_model[-1].coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import relplot as rp\n",
    "\n",
    "y_pred_total = model.predict_proba(X)\n",
    "print(\"calibration error:\", rp.smECE(y_pred_total[:, 1], y))\n",
    "fig, ax = rp.rel_diagram(y_pred_total[:, 1], y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate permutation importance per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    tuple(zip(X.columns, result[\"importances_mean\"], strict=True))\n",
    ")\n",
    "feature_importance_df.columns = [\"feature\", \"importance\"]\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "feature_importance_df.plot.barh(x=\"feature\", y=\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgboost_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame(\n",
    "    tuple(zip(X.columns, result[\"importances_mean\"], strict=True))\n",
    ")\n",
    "feature_importance_df.columns = [\"feature\", \"importance\"]\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "feature_importance_df.plot.barh(x=\"feature\", y=\"importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no_show_ruben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
