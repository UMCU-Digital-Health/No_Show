{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments\n",
    "\n",
    "This notebook is for experimenting with the choice of model and performance variations between different time periods and clinics.\n",
    "\n",
    "1. Train and evaluate current best model on total dataset (without differentiating between clinics)\n",
    "2. Apply IECV to different models and check heteregeneity with respect to clinics\n",
    "3. Train models per clinic and check performance\n",
    "4. Use clinic as categorical feature in tree based models and check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedGroupKFold,\n",
    "    TimeSeriesSplit,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, SplineTransformer\n",
    "\n",
    "from noshow.features.feature_pipeline import create_features\n",
    "from noshow.model.predict import create_prediction\n",
    "from noshow.preprocessing.load_data import (\n",
    "    load_appointment_csv,\n",
    "    process_appointments,\n",
    "    process_postal_codes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split in to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuretable = pd.read_parquet(\"../data/processed/featuretable.parquet\")\n",
    "\n",
    "featuretable[\"no_show\"] = (\n",
    "    featuretable[\"no_show\"].replace({\"no_show\": \"1\", \"show\": \"0\"}).astype(int)\n",
    ")\n",
    "featuretable[\"hour\"] = featuretable[\"hour\"].astype(\"category\")\n",
    "featuretable[\"weekday\"] = featuretable[\"weekday\"].astype(\"category\")\n",
    "\n",
    "print(featuretable.dtypes)\n",
    "\n",
    "X, y = featuretable.drop(columns=\"no_show\"), featuretable[\"no_show\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgboost_model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    categorical_features=[\"hour\", \"weekday\"],\n",
    ")\n",
    "\n",
    "categorical_features = [\"hour\", \"weekday\"]\n",
    "continuous_features = X.columns.difference(categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"continuous\",\n",
    "            Pipeline([(\"scaler\", RobustScaler()), (\"spline\", SplineTransformer())]),\n",
    "            continuous_features,\n",
    "        ),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate current best model on total dataset (without differentiating between clinics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_auc_curve(X_train, y_train, model, cv, train_groups=None, title=None):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    test_indices = {}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train, train_groups)):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_score = model.predict_proba(X_test_cv)[:, 1]\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_cv, y_score)\n",
    "        roc_auc[i] = roc_auc_score(y_test_cv, y_score)\n",
    "        test_indices[i] = test_idx\n",
    "\n",
    "        ax.plot(fpr[i], tpr[i], c=\"b\", alpha=0.15)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    # Add mean AUC and standard deviation to the legend\n",
    "    mean_auc = np.mean(list(roc_auc.values()))\n",
    "    std_auc = np.std(list(roc_auc.values()))\n",
    "    ax.legend([f\"ROC curve (AUC = {mean_auc:.3f} +/- {std_auc:.3f})\"])\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    fig.show()\n",
    "\n",
    "    return roc_auc, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    lgboost_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    log_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create modified version of create_treatment_groups\n",
    "def create_treatment_groups(\n",
    "    predictions: pd.DataFrame, patients: pd.DataFrame, bin_edges\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create treatment groups based on predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : pd.DataFrame\n",
    "        DataFrame containing predictions.\n",
    "    patients : pd.DataFrame\n",
    "        A dataframe containing the treatment groups of patients\n",
    "    bin_edges : list\n",
    "        List of edges defining the bins for prediction scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with treatment group assignments.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the predictions DataFrame is empty.\n",
    "    \"\"\"\n",
    "    if predictions.empty:\n",
    "        raise ValueError(\"The predictions DataFrame is empty.\")\n",
    "\n",
    "    # get unique patient ids\n",
    "    unique_patient_ids = predictions[\"pseudo_id\"].unique().tolist()\n",
    "    relevant_patients = patients[patients[\"id\"].isin(unique_patient_ids)]\n",
    "\n",
    "    if not relevant_patients.empty:\n",
    "        # Merge predictions with patients to get treatment group\n",
    "        predictions = pd.merge(\n",
    "            predictions,\n",
    "            relevant_patients[[\"id\", \"treatment_group\"]],\n",
    "            right_on=\"id\",\n",
    "            left_on=\"pseudo_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    else:\n",
    "        predictions.loc[:, \"treatment_group\"] = None\n",
    "\n",
    "    # Create prediction score bins using quantile bins\n",
    "    predictions = predictions.sort_values([\"prediction\"], ascending=False)\n",
    "\n",
    "    # only keep top prediction per patient for treatment/control split\n",
    "    deduplicated = predictions.drop_duplicates(subset=\"pseudo_id\", keep=\"first\")\n",
    "    # only keep patients without treatment group\n",
    "    deduplicated = deduplicated[deduplicated[\"treatment_group\"].isnull()]\n",
    "    deduplicated[\"score_bin\"] = pd.cut(deduplicated[\"prediction\"], bins=bin_edges, include_lowest=True, labels=False, right=True)\n",
    "\n",
    "    # Create stratified randomization in control and treatment groups using\n",
    "    deduplicated = deduplicated.sort_values([\"hoofdagenda\", \"prediction\"])\n",
    "    random.seed(1234)  # Set fixed seed for reproducibility\n",
    "    deduplicated[\"treatment_group\"] = deduplicated.groupby(\n",
    "        [\"hoofdagenda\", \"score_bin\"],\n",
    "        observed=True,\n",
    "        # create random treatment group assignment\n",
    "    )[\"prediction\"].transform(lambda x: (np.arange(len(x)) + random.randint(0, 1)) % 2)\n",
    "\n",
    "    # update predictions with treatment groups from deduplicated based on pseudo_id\n",
    "    merged = predictions.merge(\n",
    "        deduplicated[[\"pseudo_id\", \"treatment_group\", \"score_bin\"]],\n",
    "        on=\"pseudo_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_new\"),\n",
    "    )\n",
    "    # Update the treatment_group values where there's a new value\n",
    "    merged[\"treatment_group\"] = merged[\"treatment_group\"].fillna(merged[\"treatment_group_new\"])\n",
    "\n",
    "    deduplicated['id'] = deduplicated['pseudo_id']\n",
    "    # append deduplicated to patients\n",
    "    patients = pd.concat([patients, deduplicated[[\"id\", \"treatment_group\"]]], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    return merged, patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_df = load_appointment_csv(\"../data/raw/poliafspraken_pilot.csv\")\n",
    "appointments_df = process_appointments(appointments_df).sort_index()\n",
    "with open(\"../output/models/no_show_model_cv.pickle\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "all_postalcodes = process_postal_codes(\"../data/raw/NL.txt\")\n",
    "predictions_df = create_prediction(model, appointments_df, all_postalcodes)\n",
    "# randomly assing hoofdagenda string to predictions_df\n",
    "predictions_df[\"hoofdagenda\"] = np.random.choice([\"A\", \"B\", \"C\"], len(predictions_df))\n",
    "predictions_df = predictions_df.loc[\n",
    "    predictions_df.index.get_level_values(\"start\").year == 2022\n",
    "]\n",
    "predictions_df = predictions_df.reset_index()\n",
    "predictions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine fixed quantiles:\n",
    "# Calculate quantiles\n",
    "n_bins = 4\n",
    "quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "bin_edges = predictions_df['prediction'].quantile(quantiles).values\n",
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get unique days in predictions_df[\"start\"]\n",
    "days = predictions_df[\"start\"].dt.date.unique()\n",
    "patients = pd.DataFrame(columns=[\"id\", \"treatment_group\"])\n",
    "final_predictions = []\n",
    "for day in days:\n",
    "    day_df = predictions_df[predictions_df[\"start\"].dt.date == day]\n",
    "    predict, patients = create_treatment_groups(day_df, patients, bin_edges)\n",
    "    final_predictions.append(predict)\n",
    "final_predictions= pd.concat(final_predictions)\n",
    "print(len(patients)) == len(patients['id'].unique())\n",
    "print(final_predictions.shape)\n",
    "final_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first entry for every pseuoo_id\n",
    "first_app = final_predictions.sort_values(\"prediction\", ascending=False).drop_duplicates(subset=\"pseudo_id\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "first_app.groupby([\"hoofdagenda\", \"treatment_group\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Treatment Groups per hoofdagenda and score_bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "final_predictions.groupby([\"hoofdagenda\", \"treatment_group\", \"score_bin\"]).size().unstack().plot(kind='bar', ax=ax)\n",
    "plt.xlabel(\"hoofdagenda and score_bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Treatment Groups per hoofdagenda and score_bin\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patients['id'].nunique())\n",
    "# find users with multiple treatment groups\n",
    "patients[patients.duplicated(subset='id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(patients['id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check temporal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_timesorted = X.sort_index(level=\"start\")\n",
    "y_timesorted = y.sort_index(level=\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, test_indices = cv_auc_curve(\n",
    "    X_timesorted,\n",
    "    y_timesorted,\n",
    "    lgboost_model,\n",
    "    TimeSeriesSplit(n_splits=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_times = [\n",
    "    str(\n",
    "        (\n",
    "            X.iloc[idx].index.get_level_values(\"start\").min().strftime(\"%Y-%m-%d\"),\n",
    "            X.iloc[idx].index.get_level_values(\"start\").max().strftime(\"%Y-%m-%d\"),\n",
    "        )\n",
    "    )\n",
    "    for idx in test_indices.values()\n",
    "]\n",
    "fold_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_scores = pd.Series(roc_auc)\n",
    "roc_scores.index = fold_times\n",
    "roc_scores.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply IECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_df = load_appointment_csv(\"../data/raw/poliafspraken_no_show.csv\")\n",
    "appointments_df = process_appointments(appointments_df)\n",
    "all_postalcodes = process_postal_codes(\"../data/raw/NL.txt\")\n",
    "appointments_features = create_features(\n",
    "    appointments_df, all_postalcodes, minutes_early_cutoff=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features = (\n",
    "    appointments_features[\n",
    "        [\n",
    "            \"hoofdagenda\",\n",
    "            \"hour\",\n",
    "            \"weekday\",\n",
    "            \"minutesDuration\",\n",
    "            \"no_show\",\n",
    "            \"prev_no_show\",\n",
    "            \"prev_no_show_perc\",\n",
    "            \"age\",\n",
    "            \"dist_umcu\",\n",
    "            \"prev_minutes_early\",\n",
    "            \"earlier_appointments\",\n",
    "            \"appointments_same_day\",\n",
    "            \"appointments_last_days\",\n",
    "            \"days_since_created\",\n",
    "            \"days_since_last_appointment\",\n",
    "        ]\n",
    "    ]\n",
    "    .reset_index()\n",
    "    .set_index([\"pseudo_id\", \"start\", \"hoofdagenda\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features[\"no_show\"] = (\n",
    "    appointments_features[\"no_show\"].replace({\"no_show\": 1, \"show\": 0}).astype(int)\n",
    ")\n",
    "\n",
    "appointments_features[\"hour\"] = appointments_features[\"hour\"].astype(\"category\")\n",
    "appointments_features[\"weekday\"] = appointments_features[\"weekday\"].astype(\"category\")\n",
    "\n",
    "X, y = appointments_features.drop(columns=\"no_show\"), appointments_features[\"no_show\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = X.index.get_level_values(\"pseudo_id\")\n",
    "\n",
    "cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    lgboost_model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_leave_one_out(df):\n",
    "    groups = df.index.get_level_values(\"hoofdagenda\").unique()\n",
    "\n",
    "    for test_group in groups:\n",
    "        train_index = df.index.get_level_values(\"hoofdagenda\") != test_group\n",
    "        test_index = df.index.get_level_values(\"hoofdagenda\") == test_group\n",
    "\n",
    "        yield test_group, (np.where(train_index)[0], np.where(test_index)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iecv_auc_curve(X_train, y_train, model):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    test_indices = {}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for group, (train_idx, test_idx) in group_leave_one_out(X_train):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_score = model.predict_proba(X_test_cv)[:, 1]\n",
    "        fpr[group], tpr[group], _ = roc_curve(y_test_cv, y_score)\n",
    "        roc_auc[group] = roc_auc_score(y_test_cv, y_score)\n",
    "        test_indices[group] = test_idx\n",
    "\n",
    "        ax.plot(fpr[group], tpr[group], label=group)\n",
    "\n",
    "    print(\n",
    "        f\"Mean AUC: {np.mean(list(roc_auc.values()))}\"\n",
    "        f\"(+/- {np.std(list(roc_auc.values()))})\"\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "    return roc_auc, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iecv_auc_curve(X, y, lgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iecv_auc_curve(X, y, log_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV per poli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = appointments_features.drop(columns=\"no_show\"), appointments_features[\"no_show\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poli in X.index.get_level_values(\"hoofdagenda\").unique():\n",
    "    X_tmp = X.loc[(slice(None), slice(None), poli), :]\n",
    "    y_tmp = y.loc[(slice(None), slice(None), poli)]\n",
    "    train_groups_tmp = X_tmp.index.get_level_values(\"pseudo_id\")\n",
    "\n",
    "    _, _ = cv_auc_curve(\n",
    "        X_tmp,\n",
    "        y_tmp,\n",
    "        lgboost_model,\n",
    "        # HistGradientBoostingClassifier(learning_rate=0.05, max_iter=300),\n",
    "        StratifiedGroupKFold(n_splits=5),\n",
    "        train_groups_tmp,\n",
    "        title=poli,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding poli as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments_features_agenda = appointments_features.reset_index()\n",
    "appointments_features_agenda[\"hoofdagenda_cat\"] = appointments_features_agenda[\n",
    "    \"hoofdagenda\"\n",
    "].astype(\"category\")\n",
    "appointments_features_agenda = appointments_features_agenda.set_index(\n",
    "    [\"pseudo_id\", \"start\", \"hoofdagenda\"]\n",
    ")\n",
    "X, y = (\n",
    "    appointments_features_agenda.drop(columns=\"no_show\"),\n",
    "    appointments_features_agenda[\"no_show\"],\n",
    ")\n",
    "\n",
    "model = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    categorical_features=[\"hour\", \"weekday\", \"hoofdagenda_cat\"],\n",
    ")\n",
    "train_groups = X.index.get_level_values(\"pseudo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = cv_auc_curve(\n",
    "    X,\n",
    "    y,\n",
    "    model,\n",
    "    StratifiedGroupKFold(n_splits=5),\n",
    "    train_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = featuretable.drop(columns=\"no_show\"), featuretable[\"no_show\"]\n",
    "\n",
    "model = log_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(log_model[-1].coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import relplot as rp\n",
    "\n",
    "y_pred_total = model.predict_proba(X)\n",
    "print(\"calibration error:\", rp.smECE(y_pred_total[:, 1], y))\n",
    "fig, ax = rp.rel_diagram(y_pred_total[:, 1], y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate permutation importance per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    tuple(zip(X.columns, result[\"importances_mean\"], strict=True))\n",
    ")\n",
    "feature_importance_df.columns = [\"feature\", \"importance\"]\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "feature_importance_df.plot.barh(x=\"feature\", y=\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgboost_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame(\n",
    "    tuple(zip(X.columns, result[\"importances_mean\"], strict=True))\n",
    ")\n",
    "feature_importance_df.columns = [\"feature\", \"importance\"]\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "feature_importance_df.plot.barh(x=\"feature\", y=\"importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no_show_ruben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
